<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Kogito Tooling 0.11.0 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FE87raV3XMI/kogito-tooling-0-11-0-released.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/07/kogito-tooling-0-11-0-released.html</id><updated>2021-07-09T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.11.0 , we made a lot of improvements and bug fixes. We are also happy to announce that this release marks the first release of our DMN Runner on ! This post will give a quick overview of this . I hope you enjoy it! INSTANTANEOUS FEEDBACK LOOP FOR DMN AUTHORING WITH DMN RUNNER We’ve been exploring ways to augment the developer experience for our editors on the KIE Tooling team. Today, I’m proud to announce the release of our DMN Runner on dmn.new. We hope it will transform your DMN Authoring experience. Let’s see it in action: For this reason, we decided to start a new development stream for our BPMN, DMN, and Scenario Simulator editors called , freeing the way for Editors to continue evolving on both streams separately (Kogito and BC) without carrying the weight of the other. Soon we will write a blog post describing this new awesome feature. NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS We also made some new features, a lot of refactorings and improvements, with highlights to: * – Online Editor DMN Runner (First Iteration) * – The nodes should be created on top of the selected node in DMN editor * – BPMN Editor – Improve SVG generated ids * – [DMN Designer] Read-only mode – Connectors appear differently on read-only mode * – Implement integration tests for Save operation in BPMN editor in VSCode * – [SceSim Designer] HiDPI is not working as expected * – [DMN/BPMN] Sync kogito-editors-java with latest translations * – Kogito Tooling VS Code extensions Workspaces Trust * – Prevent different envelopes in the same window to interact with each other * – New elements should always be connected by their central magnetic point * – Stunner – Palette fixes &amp;amp; improvements * – Guided tour for invalid DMN models * – It’s not possible to save arrow edits * – SceSim Editor does not work in Eclipse Theia * – [BC included] [DMN/BPMN editor] Sometimes clicking outside doesn’t unselect nodes * – Stunner – Texts overlap toolboxes * – [BC Included] DMN editor removing edges for duplicate Decision Nodes on canvas * – Inconsistent results of integration tests across CI * – Stunner – Unknown Custom tasks in Designer makes Diagram Explorer empty * – Clear selection button doesn’t work on Testing Tools when use click property first time. * – Stunner – Editing text using Inline editor is shown over Properties panel or Expanded Palette * – Stunner – The order of Custom tasks in the palette is different with every process opening FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog that I recommend you read: * , by Kirill Gaevskii; * , by Guilherme Carreiro; * , by Valentino Pellegrino; We also presented in some Kie Lives: * , by William Siqueira; * , by Guilherme Carreiro; * , by Guilherme Caponetto. THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! [kie] The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FE87raV3XMI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/kogito-tooling-0-11-0-released.html</feedburner:origLink></entry><entry><title type="html">RESTEasy 4.7.0.Final is now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PW_KlSk8v_M/" /><author><name /></author><id>https://resteasy.github.io/2021/07/08/resteasy-4.7.0.Final/</id><updated>2021-07-08T21:49:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PW_KlSk8v_M" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/08/resteasy-4.7.0.Final/</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iURyZgi6LDM/troubleshooting-application-performance-red-hat-openshift-metrics-part-1" /><author><name>Pavel Macik</name></author><id>951504be-916c-414c-b971-9f2b68403bd1</id><updated>2021-07-08T07:00:00Z</updated><published>2021-07-08T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first In a series of five articles showing how developers can use an extensive set of metrics offered by &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to diagnose and fix performance problems. I'll describe a real-life success story where I did performance testing on the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator &lt;/a&gt; to get it accepted into the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. I'll describe the Service Binding Operator's performance challenges, how I planned my troubleshooting, and how I created and viewed the metrics.&lt;/p&gt; &lt;p&gt;This first article lays out the motivation for the whole effort, the Service Binding Operator's environment and testing setup, the requirements I had to meet to get the Service Binding Operator accepted into the Developer Sandbox, and the tooling made available by Developer Sandbox for performance testing.&lt;/p&gt; &lt;h2&gt;Motivation&lt;/h2&gt; &lt;p&gt;My team wanted to make the Service Binding Operator available on the Developer Sandbox for Red Hat OpenShift. Our goal was to use the operator in a demo and workshop titled &lt;a href="developer-sandbox/activities/connecting-to-your-managed-kafka-instance" target="_blank"&gt;Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt; at the April 2021 &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog"&gt;Red Hat Summit&lt;/a&gt;. We also think this operator is useful to developers experimenting with the sandbox.&lt;/p&gt; &lt;p&gt;One of the requirements to get the Service Binding Operator accepted into the Developer Sandbox was to pass a performance evaluation. This evaluation would basically ensure that the operator wouldn't crash the Developer Sandbox while being invoked by a reasonable load of active users.&lt;/p&gt; &lt;h2&gt;Environment: The Developer Sandbox&lt;/h2&gt; &lt;p&gt;Technically, the Developer Sandbox is a couple of operators installed on an ordinary OpenShift cluster, which is an instance of the &lt;a href="https://www.openshift.com/products/dedicated/" target="_blank"&gt;Red Hat OpenShift Dedicated&lt;/a&gt; managed, cloud-based service. Developer Sandbox is scaled to support many concurrent users and their activity. For each developer (active user) registered on the sandbox, two namespaces are created to help the developer try out, play with, and learn about the OpenShift environment.&lt;/p&gt; &lt;p&gt;From the perspective of the Service Binding Operator, the Developer Sandbox is just a regular OpenShift instance. So, making the Service Binding Operator available to Developer Sandbox users is a simple matter of installing the operator into the underlying OpenShift cluster.&lt;/p&gt; &lt;p&gt;There are two ways to install an operator into the sandbox's cluster. The first way takes advantage of the &lt;code&gt;redhat-operators&lt;/code&gt; catalog source, which is available out of the box in OpenShift through the &lt;a href="https://docs.openshift.com/container-platform/4.5/operators/understanding/olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager&lt;/a&gt;. That catalog source hosts the official Red Hat releases of the Service Binding Operator.&lt;/p&gt; &lt;p&gt;The second installation method is specific to OpenShift Dedicated, which offers add-ons for Red Hat tools. But because Service Binding Operator is still a technology preview (not yet generally available), there is no OpenShift Dedicated add-on for it. So, we decided to go with the first option and install the Service Binding Operator via the in-cluster Operator Hub from the official catalog source.&lt;/p&gt; &lt;h2&gt;Developer Sandbox requirements and limitations&lt;/h2&gt; &lt;p&gt;The Developer Sandbox team specified several requirements that the operator had to meet to be accepted and installed on the production instance of Developer Sandbox. Some were operational, but the ones relevant to this series are related to performance.&lt;/p&gt; &lt;h3&gt;Operational requirements&lt;/h3&gt; &lt;p&gt;These requirements address the Service Binding Operator's integration into OpenShift Dedicated and the Developer Sandbox.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The operator must not require the creation of any additional namespaces other than its own.&lt;/li&gt; &lt;li&gt;It must be available on OpenShift Dedicated to run on OpenShift Dedicated clusters. (I'll show how to upload the Service Binding Operator in the next article in this series.)&lt;/li&gt; &lt;li&gt;It must be able to operate with the &lt;a href="https://github.com/redhat-developer/app-services-operator"&gt;Red Hat OpenShift Application Services&lt;/a&gt; Operator.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Performance requirements&lt;/h3&gt; &lt;p&gt;The remainder of this series focuses on what I did to meet the performance requirements for the Developer Sandbox. Essentially, we could have only one instance of the Service Binding Operator in the whole cluster, and that operator had to be able to handle a maximum of 3,000 users per cluster, as well as up to 10,000 namespaces. (Consider that Service Binding Operator requires a namespace, and that each user gets two more.)&lt;/p&gt; &lt;p&gt;As a result, a Developer Sandbox cluster consumes a lot of Kubernetes or OpenShift resources. The 3,000 users and up to 10,000 namespaces can lead to up to 100,000 role bindings, hundreds of thousands of secrets, and thousands of config maps, pods, deployments, build configs, and so on, per cluster. The operator must be able to handle that load without consuming too many compute resources, which would compromise cluster stability.&lt;/p&gt; &lt;h2&gt;Tooling for performance evaluations&lt;/h2&gt; &lt;p&gt;The Developer Sandbox team has its own &lt;a href="https://github.com/codeready-toolchain/toolchain-e2e/blob/master/setup/README.adoc" target="_blank"&gt;setup tool&lt;/a&gt;, which was originally used to test the Developer Sandbox itself. The tool was made available for other operator teams for conducting performance evaluations. The only prerequisite is to have access to an OpenShift cluster of the scale equivalent to what we need in production where the tested operator (the Service Binding Operator and Red Hat OpenShift Application Services Operator in our case) is installed and running. The tool can then do the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Install the sandbox on the OpenShift cluster.&lt;/li&gt; &lt;li&gt;Simulate a specified &lt;em&gt;x&lt;/em&gt; number of developers registering into the sandbox and simulate &lt;em&gt;y&lt;/em&gt; of the &lt;em&gt;x&lt;/em&gt; developers as active, creating workloads in their namespaces. For example, we could simulate a cluster where 3,000 users are registered and 1,000 of them are active.&lt;/li&gt; &lt;li&gt;Clean the cluster if necessary.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The tool creates a default set of workloads in one namespace of each active user and makes it possible to add custom workloads (typically specific for the tested operator usage) in the active users' simulations.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;This article has laid out the performance requirements for the Service Binding Operator to be to accepted to the Developer Sandbox for Red Hat OpenShift. The rest of this series documents the performance journey through the following tasks:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Provision the OpenShift cluster&lt;/li&gt; &lt;li&gt;Install the sandbox into the OpenShift cluster&lt;/li&gt; &lt;li&gt;Install Service Binding Operator and the Red Hat OpenShift Application Services Operator into the OpenShift cluster&lt;/li&gt; &lt;li&gt;Simulate active users "using the Service Binding Operator"&lt;/li&gt; &lt;li&gt;Collect runtime metrics from OpenShift&lt;/li&gt; &lt;li&gt;Extract and compute the test results&lt;/li&gt; &lt;li&gt;Compile a report&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Stay tuned for Part 2, where we will set up the testing environment and simulations.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 1: Requirements&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iURyZgi6LDM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-08T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1</feedburner:origLink></entry><entry><title type="html">Shopping recommendations in PMML.</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Koi2zA6yuvs/shopping-recommendations-in-pmml.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2021/07/shopping-recommendations-in-pmml.html</id><updated>2021-07-07T07:28:36Z</updated><content type="html">In previous posts ( and ) we had a glance at how a PMML engine has been implemented inside Drools/Kogito ecosystem. This time we will start looking at a concrete example of a recommendation engine based on top of PMML. The first part of this post will deal with the ML aspect of it, and the second part will talk of the actual usage of the generated model. So, let’s start… USE CASE DESCRIPTION The ABC Inc. wants to increase its sales. In the past, they already tried to suggest the product they would like to sell but without a good result. Recently, they heard about AI so they want to use a more advanced approach where the suggested product is not decided upfront but it is defined based on the behaviour of the users. The company sells three kinds of items: Books, PCs, and Cars, but usually the customers buy one type of item, seldom the others. So, the company wants to recommend items of the preferred type and not ones that have already been bought. DATA PREPARATION AND MODEL CREATION The code used to generate data and the PMML model is available at repository.  For simplicity, there are ten items for each type, and they are named Book-0, Book-1, Book-2 etc. The script is used to randomly generate a sample of 1000 customers that preferentially buy one kind of item, and possibly some other items of different types. A 30-dimensional array represents the purchased items, where 1 means a bought item. The first 10 elements are the Books, the next 10 elements are the Cars, and the last 10 elements are the PCs. BookCarPC0, 1,…,910, 11,…, 1920, 21, … 29 Generated data are stored inside   We have chosen to use a cluster model to represent the distribution of customers in the three main buyer groups (Books, Cars, PCs). To define, train and test the model, the environment has been used. It provides a ML environment bound to a GitHub repository. A quick and clear tutorial of its usage with Python may be found   For the model training, part of the generated data has been extracted.  This shows both data and graphical output: clusterpredictionbuyer group5191Car8372Book2080PC5251Car9780PC The last step is the actual dump of the generated model to PMML format, done with the library.  Here’s the generated TRUSTY-PMML PRIMER Trusty-PMML offers an easy-to-use API to evaluate models against given input. First of all, a reference to a model-specific PMMLRuntime has to be created. Available methods are defined inside interface. Some of them are meant to be used inside a KieServer container, with a pre-generated Kjar that contains the model already compiled to java classes.  Others simply require a working reference to the PMML file, in which case the model will be compiled in-memory the first time it is executed. The following snippet shows an example of the latter: ClassLoader classloader = Thread.currentThread().getContextClassLoader(); URL pmmlUrl = classloader.getResource(PMML_FILENAME); File pmmlFile = FileUtils.getFile(pmmlUrl.getFile()); PMMLRuntime pmmlRuntime = new PMMLRuntimeFactoryImpl().getPMMLRuntimeFromFile(pmmlFile); The next step requires the creation of a PMMLContext containing the input data to use for evaluation. The following snippet shows an example of that: static PMMLContext getPMMLContext(String modelName, Map&lt;String, Object&gt; inputData) { String correlationId = "CORRELATION_ID"; PMMLRequestDataBuilder pmmlRequestDataBuilder = new PMMLRequestDataBuilder(correlationId, modelName); for (Map.Entry&lt;String, Object&gt; entry : parameters.entrySet()) { Object pValue = entry.getValue(); Class class1 = pValue.getClass(); pmmlRequestDataBuilder.addParameter(entry.getKey(), pValue, class1); } PMMLRequestData pmmlRequestData = pmmlRequestDataBuilder.build(); return new PMMLContextImpl(pmmlRequestData); } The last step it is to actually retrieve the evaluation of the model inside a PMML4Result: PMML4Result result = pmmlRuntime.evaluate(modelName, pmmlContext); PMML4Result contains the status of the execution (resultCode), the  name of the target field (resultObjectName), and all the variables evaluated by the model. The following snippet shows how to retrieve the target results: String targetField = pmml4Result.getResultObjectName(); Object result = pmml4Result.getResultVariables().get(targetField); THE RECOMMENDER ENGINE The uses the “cluster_buyer_predictor.pmml” with the engine to predict the “buyer-group” of a randomly created customer. The pmml engine is responsible for making such predictions.  Based on that, the rest of the code will identify which items have not been already purchased by the customer, and will suggest one of them.  The object is a simple DTO that is initialized with randomly selected preferred type and bought items, stored as List. public class Customer { private final List&lt;String&gt; buyedItems; public Customer() { ItemType itemType = ItemType.byClusterId(new Random().nextInt(3)); buyedItems = mainBuyedItems(itemType); buyedItems.addAll(casualBuys(itemType)); } The loop in the creates five customers and retrieves the recommendation for each of them. public static void main(String[] args) { IntStream.range(0, 5).forEach(i -&gt; { Customer customer = new Customer(); logger.info("Customer {}", customer); String recommendation = getRecommendation(customer); logger.info("We recommend: {}", recommendation); }); } class is the core of the application. It invokes method to translate the List of bought items to a 30-dimensional array of integers (0 and 1): int[] buyedItems = Converter.getBuyedItemsIndexes(customer); Then, it calls method to retrieve the cluster the customer belongs to: int clusterId = PMMLUtils.getClusterId(buyedItems); Last, based on the cluster id and the already bought items, a recommendation is generated: private static String getRecommendation(ItemType itemType, List&lt;String&gt; buyedItems) { logger.info("getRecommendation {} {}", itemType, buyedItems); List&lt;String&gt; alreadyBuyed = buyedItems .stream() .filter(buyed -&gt; buyed.startsWith(itemType.getClusterName())) .collect(Collectors.toList()); if (alreadyBuyed.size() == 10) { return null; } return IntStream.range(0, 10) .mapToObj(i -&gt; itemType.getClusterName() + "-" + i) .filter(itemName -&gt; !alreadyBuyed.contains(itemName)) .findFirst() .orElse(null); } , on the other hand, is where the PMML model is actually instantiated and evaluated, so let’s dive deeper into it. To start with, a static block initialize the PMMLRuntime, reading the pmml file: static { ClassLoader classloader = Thread.currentThread().getContextClassLoader(); final URL pmmlUrl = classloader.getResource(PMML_FILENAME); File pmmlFile = FileUtils.getFile(pmmlUrl.getFile()); PMML_RUNTIME = new PMMLRuntimeFactoryImpl().getPMMLRuntimeFromFile(pmmlFile); } The getClusterId method convert the int[] to a Map; for each element in the array, a new entry is created, with the index of the element as key, and the value of the element (cast to double) as value: public static int getClusterId(int[] buyedItems) { logger.info("getClusterId {}", buyedItems); Map&lt;String, Object&gt; inputData = new HashMap&lt;&gt;(); for (int i = 0; i &lt; buyedItems.length ; i ++) { inputData.put(String.valueOf(i), (double) buyedItems[i]); } PMML4Result pmml4Result = evaluate(PMML_RUNTIME, inputData, MODEL_NAME); logger.info("pmml4Result {}", pmml4Result); String clusterIdName = (String) pmml4Result.getResultVariables().get(OUTPUT_FIELD); return Integer.parseInt(clusterIdName); } The evaluate method creates a PMMLContext out of the provided Map and returns the result of the evaluation, as PMML4Result : private static PMML4Result evaluate(final PMMLRuntime pmmlRuntime, final Map&lt;String, Object&gt; inputData, final String modelName) { logger.info("evaluate with PMMLRuntime {}", pmmlRuntime); final PMMLRequestData pmmlRequestData = getPMMLRequestData(modelName, inputData); final PMMLContext pmmlContext = new PMMLContextImpl(pmmlRequestData); return pmmlRuntime.evaluate(modelName, pmmlContext); } SUM UP In this post we have tackled a real-world recommendation scenario using the PMML and Trusty-PMML engine. First, we have created some sample data and trained a KMeans cluster model out of them. Then, we have provided a brief explanation on the basic Trusty-PMML API. Last, we have shown a bare-bone java project that, featuring the pmml engine, is able to provide reliable recommendations. But this is just the start of the journey. In the next posts we will see how to implement a cloud-native service that remotely provides the required predictions, and then… but let’s not spoil the surprise. Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Koi2zA6yuvs" height="1" width="1" alt=""/&gt;</content><dc:creator>Gabriele Cardosi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/shopping-recommendations-in-pmml.html</feedburner:origLink></entry><entry><title>Deploy .NET applications on Red Hat OpenShift using Helm</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Xq7zriv-M6U/deploy-net-applications-red-hat-openshift-using-helm" /><author><name>Tom Deseyn</name></author><id>3192b6f7-ec34-479d-9843-9ead1aa63a3f</id><updated>2021-07-07T07:00:00Z</updated><published>2021-07-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://helm.sh/"&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. It provides an easy way to deploy a set of resources on a Kubernetes or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; clusters. This article starts with a quick introduction to Helm. Then we'll use it to deploy a .NET application.&lt;/p&gt; &lt;h2&gt;Overview of Helm&lt;/h2&gt; &lt;p&gt;A Helm &lt;em&gt;chart&lt;/em&gt; describes a set of resources to deploy. The chart uses templates that can be configured on the command line or through a YAML file.&lt;/p&gt; &lt;p&gt;Charts can be distributed by hosting them on an HTTP server called a &lt;em&gt;chart repository&lt;/em&gt;. The &lt;code&gt;helm&lt;/code&gt; executable can fetch charts from the repository.&lt;/p&gt; &lt;p&gt;A Helm chart deployed on Kubernetes is called a &lt;em&gt;release&lt;/em&gt;. After you install the initial release, it can be updated. Each update has a corresponding release number. The release can also be rolled back to an earlier version.&lt;/p&gt; &lt;h2&gt;Helm prerequisites and setup&lt;/h2&gt; &lt;p&gt;To use Helm, you need the &lt;code&gt;helm&lt;/code&gt; binary and an OpenShift cluster.&lt;/p&gt; &lt;p&gt;You can download the binary from the &lt;a href="https://github.com/helm/helm/releases"&gt;GitHub release&lt;/a&gt; page. After you’ve extracted the tarball, add the directory containing the command to your PATH environment variable. You can check that the command works by executing &lt;code&gt;helm version&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm version version.BuildInfo{Version:"v3.5.4", GitCommit:"1b5edb69df3d3a08df77c9902dc17af864ff05d1", GitTreeState:"clean", GoVersion:"go1.15.11"}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the &lt;code&gt;oc&lt;/code&gt; client command to log in to your OpenShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc login --token=xxx --server=https://yyy&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are not familiar with the OpenShift client, check out the &lt;a href="https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html"&gt;Getting started with the OpenShift CLI&lt;/a&gt; documentation.&lt;/p&gt; &lt;p&gt;For our example, we’ll use the .NET Core 3.1 and .NET 5.0 versions. You can check which versions are available globally using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get -n openshift is dotnet --template='{{range .spec.tags}}{{.name}}{{"\n"}}{{end}}' 2.1 2.1-el7 2.1-ubi8 3.1 3.1-el7 3.1-ubi8 latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that the .NET 5 version is not available on my cluster. Because I am not an administrator. I will import the images into my project namespace using the Bash shell script &lt;code&gt;install-imagestreams.sh&lt;/code&gt; from the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;s2i-dotnetcore&lt;/a&gt; repository. The repository has a &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore#windows"&gt;PowerShell script&lt;/a&gt;, too:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ wget https://raw.githubusercontent.com/redhat-developer/s2i-dotnetcore/master/install-imagestreams.sh $ chmod +x install-imagestreams.sh $ ./install-imagestreams.sh --os rhel&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the shell script finishes, you can check the versions available in the project namespace by running the previous command without the &lt;code&gt;-n openshift&lt;/code&gt; argument:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get is dotnet --template='{{range .spec.tags}}{{.name}}{{"\n"}}{{end}}' 2.1 2.1-el7 2.1-ubi8 3.1 3.1-el7 3.1-ubi8 5.0 5.0-ubi8 latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that .NET 5.0 is available now.&lt;/p&gt; &lt;h2&gt;Using the .NET Helm chart&lt;/h2&gt; &lt;p&gt;First, we’ll configure Helm to fetch templates from the &lt;code&gt;redhat-helm-charts&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm repo add redhat-charts https://redhat-developer.github.io/redhat-helm-charts&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can see the available charts using the &lt;code&gt;helm search&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm search repo redhat-charts NAME CHART VERSION ... redhat-charts/dotnet 0.0.1 ... redhat-charts/nodejs 0.0.1 ... redhat-charts/quarkus 0.0.3 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get more information about the &lt;code&gt;dotnet&lt;/code&gt; chart, you can use the &lt;code&gt;helm inspect&lt;/code&gt; command. The &lt;code&gt;readme&lt;/code&gt; subcommand displays a description of the template. The &lt;code&gt;values&lt;/code&gt; subcommand shows the configuration values of the template:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm inspect readme redhat-charts/dotnet | less ... You must change the `build.uri`, `build.ref` and `build.startupProject` to refer to your own application. `build.imageStreamTag.name` must be set to match the .NET version used by your application. ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I showed here, from the output of the &lt;code&gt;readme&lt;/code&gt; subcommand, the line that describes the mandatory values to set.&lt;/p&gt; &lt;h3&gt;Deploy the .NET Core 3.1 application&lt;/h3&gt; &lt;p&gt;Let’s now use the chart to deploy the .NET Core 3.1 version of the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore-ex"&gt;s2i-dotnetcore-ex&lt;/a&gt; sample application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm install mydotnetapp \ --set build.uri=https://github.com/redhat-developer/s2i-dotnetcore-ex \ --set build.ref=dotnetcore-3.1 \ --set build.startupProject=app \ --set build.imageStreamTag.name=dotnet:3.1 \ --set build.imageStreamTag.useReleaseNamespace=true \ redhat-charts/dotnet NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:22:14 2021 NAMESPACE: demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Your .NET app is building! To view the build logs, run: oc logs bc/mydotnetapp --follow Note that your Deployment will report "ErrImagePull" and "ImagePullBackOff" until the build is complete. Once the build is complete, your image will be automatically rolled out.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first argument (&lt;code&gt;mydotnetapp&lt;/code&gt;) is the name for our release. We’ve specified the Git repository (&lt;code&gt;build.uri&lt;/code&gt;), the Git branch name (&lt;code&gt;build.ref&lt;/code&gt;), and the location of the .NET project file in the repository (&lt;code&gt;build.startupProject&lt;/code&gt;). Using &lt;code&gt;build.imageStreamTag.name&lt;/code&gt;, we selected the &lt;a href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;s2i-dotnetcore&lt;/a&gt; image for .NET 3.1 that is used to build and run our application.&lt;/p&gt; &lt;p&gt;You can visually follow the build's progress in the OpenShift web console, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dotnet_app.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dotnet_app.png?itok=tzWMW9mF" width="600" height="416" alt="The Resources tab of the OpenShift console shows the status of the application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Resources tab for the application in the OpenShift console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Update the release to .NET 5&lt;/h3&gt; &lt;p&gt;Using the &lt;code&gt;helm upgrade&lt;/code&gt; command, you can make changes to the release. Let’s change the release to the .NET 5 version.&lt;/p&gt; &lt;p&gt;As we saw during the prerequisites, my cluster didn’t include a .NET 5 image. To use the image that was installed into the project namespace, I’ve added the argument &lt;code&gt;--set build.imageStreamTag.useReleaseNamespace=true&lt;/code&gt; to the &lt;code&gt;upgrade&lt;/code&gt; subcommand:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm upgrade mydotnetapp \ --set build.ref=dotnet-5.0 \ --set build.imageStreamTag.name=dotnet:5.0 \ --set build.imageStreamTag.useReleaseNamespace=true \ redhat-charts/dotnet Release "mydotnetapp" has been upgraded. Happy Helming! NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:25:39 2021 NAMESPACE: demo STATUS: deployed REVISION: 2 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of specifying these values on the command line, you can read them from a YAML file. For example, the values could be placed in a &lt;code&gt;values.yaml&lt;/code&gt; file that lives in a &lt;code&gt;.helm&lt;/code&gt; folder next to the .NET &lt;code&gt;csproj&lt;/code&gt; project file. The &lt;code&gt;values.yaml&lt;/code&gt; file can be checked into version control to track changes and to make the file available to other developers deploying the application. For our current release, we’re using the following values:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;build: uri: https://github.com/redhat-developer/s2i-dotnetcore-ex startupProject: app ref: dotnet-5.0 imageStreamTag: name: dotnet:5.0 useReleaseNamespace: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The filename can be passed to &lt;code&gt;helm&lt;/code&gt; using the &lt;code&gt;-f&lt;/code&gt; argument:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm upgrade mydotnetapp -f values.yaml redhat-charts/dotnet Release "mydotnetapp" has been upgraded. Happy Helming! NAME: mydotnetapp LAST DEPLOYED: Thu May 20 10:32:33 2021 NAMESPACE: demo STATUS: deployed REVISION: 3 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Changes to a &lt;code&gt;BuildConfiguration&lt;/code&gt; do not automatically trigger a new build. You need to start it manually:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc start-build mydotnetapp&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;View and configure the Helm chart in OpenShift&lt;/h3&gt; &lt;p&gt;The OpenShift console recognizes Helm charts. Under the &lt;strong&gt;Helm&lt;/strong&gt; tab, you can navigate to your chart and see the three revisions you’ve deployed, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webconsole_helm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/webconsole_helm.png?itok=na7Y2rBx" width="600" height="224" alt="The application's Revision History tab shows three revisions, and shows which is currently deployed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The application's revision history in the OpenShift console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Under the &lt;strong&gt;Actions&lt;/strong&gt; dropdown menu, you can remove the release completely or roll back to an earlier version. These operations can also be performed from the terminal using the &lt;code&gt;helm history&lt;/code&gt; and &lt;code&gt;helm rollback&lt;/code&gt; commands.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;dotnet&lt;/code&gt; Helm chart provides many configuration settings that make it usable for a wide range of .NET applications. These settings support adding probes, sidecar containers, and more. If you want to do something that is not supported by the chart, you can download the chart into your source repository and customize it. When running &lt;code&gt;helm install&lt;/code&gt; or &lt;code&gt;helm upgrade&lt;/code&gt;, you can point to the chart that lives with your sources:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ helm pull --untardir charts --untar redhat-charts/dotnet $ git add charts ... make some changes to the chart at charts/dotnet … $ helm upgrade mydotnetapp -f values.yaml charts/dotnet&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has illustrated the purpose and use of Helm. We went through the steps of deploying and updating a .NET application on OpenShift using the .NET helm chart from &lt;a href="https://github.com/redhat-developer/redhat-helm-charts"&gt;redhat-helm-charts&lt;/a&gt;. To learn more about Helm, you can read the &lt;a href="https://helm.sh/docs/"&gt;Helm documentation&lt;/a&gt;. To learn more about the .NET Helm chart, you can run the &lt;code&gt;helm inspect readme redhat-charts/dotnet&lt;/code&gt; command.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm" title="Deploy .NET applications on Red Hat OpenShift using Helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Xq7zriv-M6U" height="1" width="1" alt=""/&gt;</summary><dc:creator>Tom Deseyn</dc:creator><dc:date>2021-07-07T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.1.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ejBYiKf9gCU/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-1-final-released/</id><updated>2021-07-07T00:00:00Z</updated><content type="html">We just released Quarkus 2.0.1.Final, our first maintenance release on top of 2.0. It is a safe upgrade for anyone already using 2.0.0.Final. If you are not using 2.0 already, please refer to the 2.0 migration guide. Full changelog You can get the full changelog of 2.0.1.Final on GitHub. Come...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ejBYiKf9gCU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-1-final-released/</feedburner:origLink></entry><entry><title type="html">Quarkus 2, RESTEasy 4.6 fixes and more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tH7dK2zn5qw/" /><author><name /></author><id>https://resteasy.github.io/2021/07/06/resteasy-4.6.2.Final/</id><updated>2021-07-06T00:45:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tH7dK2zn5qw" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/06/resteasy-4.6.2.Final/</feedburner:origLink></entry><entry><title type="html">Apache Camel 3.11 What's New</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gkaO8XYtwO0/apache-camel-311-whats-new.html" /><author><name>Claus Ibsen</name></author><id>http://feedproxy.google.com/~r/ApacheCamel/~3/DYxmwGm8nIw/apache-camel-311-whats-new.html</id><updated>2021-07-04T18:57:00Z</updated><content type="html">Apache Camel 3.11 has just been released. This is a LTS release which will be supported for 1 year with regular patch and security releases. This blog post first details the noteworthy changes since the last 3.10 release from last month. For readers that are upgrading from the last 3.7 LTS release then we have added a summary section that highlights all the important new features and changes (3.7 to 3.11). At first what did we do since the 3.10 release. SO WHAT'S IN THIS RELEASE SINCE 3.10 This release introduces a set of new features and noticeable improvements that we will cover in this blog post. KAMELETS Kamelets is a higher level building blocks that we keep innovating and improve over the coming releases. For Camel 3.11 we worked on making Kamelets universal across the various runtimes such as standalone, Karaf, Spring Boot, and Quarkus. We added a new camel-kamelet-main component that is intended for developers to try out or develop custom Kamelets. This module runs standalone which is intentional as we want to ensure Kamelets are not tied to a specific runtime (or the cloud on Kubernetes) but are truly universal in any environment where you can use Camel. You can find an example with camel-kamelet-main at The YAML DSL has improved error reporting when parsing to better report to Camel end users where the problem is. COMMON SOURCE TIMESTAMP We added a `getSourceTimestamp` API on `Message` to get hold of the timestamp from the source of the message. The idea is to have a common API across all the Camel components that has a timestamp of the event (such as JMS, Kafka, AWS, File/FTP etc). CLOUD COMPONENT The Camel AWS, Azure, and HuaweiCloud components have had various bug fixes and smaller improvements. QUARKUS This release is the baseline for Quarkus 2 support which is to follow shortly after this release with a new Camel Quarkus release. SPRING BOOT We have upgraded to latest Spring Boot 2.5.1 release. NO OSGI CODE IN MAIN PROJECT We had about six remaining Camel components which had some special OSGi Java source code. The OSGi code has been ported over to the Camel Karaf project. BETTER JAVA 16 SUPPORT Although Java 16 is not officially supported, we did improve a few Camel components to make them work with Java 16. The official support is Java 11 (primary) and Java 8 (secondary). NEW COMPONENTS This release has a number of new components, data formats and languages: * camel-huaweicloud-functiongraph - To call serverless functions on Huawei Cloud * camel-huaweicloud-iam - To securely manage users on Huawei Cloud * camel-kamelet-main - Main to run Kamelet standalone * camel-resourceresolver-github - Resource resolver to load files from GitHub UPGRADING Make sure to read the if you are upgrading from a previous Camel version. RELEASE NOTES You can find more information about this release in the , with a list of JIRA tickets resolved in the release. SUMMARY OF CHANGES SINCE THE LAST 3.7 LTS RELEASE It is 6 months since the last 3.7 LTS release, and here is a high level summary of the most significant changes we have done: * Optimized core (faster startup and quicker routing engine) * Modularized core (even smaller core) * Reduced Object Allocations (lower memory footprint)   * Reflection free (Native compilation friendly) * Optimized toD EIP for messaging based components * Better startup and shutdown logging * Java Flight Recorder * Routes loader (Java, XML, YAML, Groovy, JavaScript, and Kotlin) * YAML DSL * Kamelets * 17 new components * Support for Spring Boot 2.5 and Quarkus 2.0 There are many other great new features and improvements that you can find detailed in each of the Whats New blog posts: * * *&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gkaO8XYtwO0" height="1" width="1" alt=""/&gt;</content><dc:creator>Claus Ibsen</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/DYxmwGm8nIw/apache-camel-311-whats-new.html</feedburner:origLink></entry><entry><title>Making Java programs cloud-ready, Part 4: Optimize the runtime environment</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6U-wqp9ynHw/making-java-programs-cloud-ready-part-4-optimize-runtime-environment" /><author><name>Mauro Vocale</name></author><id>129d919a-4527-4951-b9a0-c2fe66a425a4</id><updated>2021-07-02T07:00:00Z</updated><published>2021-07-02T07:00:00Z</published><summary type="html">&lt;p&gt;This is the final article in a series where we are updating a monolithic &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java EE&lt;/a&gt; application to function as a &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice&lt;/a&gt; and run in a distributed cloud environment such as &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. In the first article, we set up the legacy Java application and defined our goals. Then, we upgraded the Java environment to &lt;a href="https://jakarta.ee/"&gt;Jakarta EE&lt;/a&gt;. In the last article, we used &lt;a href="https://microprofile.io/"&gt;MicroProfile&lt;/a&gt; to prepare the application for use in a distributed environment.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the whole series&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p class="Indent1"&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2021/06/25/making-java-programs-cloud-ready-part-1-incremental-approach-using-jakarta-ee"&gt;An incremental approach using Jakarta EE and MicroProfile&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/06/28/making-java-programs-cloud-ready-part-2-upgrade-legacy-java-application-jakarta"&gt;Upgrade the legacy Java application to Jakarta EE&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/06/30/making-java-programs-cloud-ready-part-3-integrate-microprofile-services"&gt;Integrate MicroProfile services&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Part 4&lt;/strong&gt;: Optimize the runtime environment&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We now have all the functionality we planned to add to our cloud-ready Java application. However, the resulting image is substantially larger than our initial image. This is not optimal because we'll need to transfer the image over the network and run it on a platform-as-a-service (PaaS) in the cloud. Resources such as memory, CPU, and RAM use factor into the costs charged by a PaaS provider. In this final article, we'll optimize the runtime to reduce the image's size and memory footprint. The benefits of optimizing the runtime include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Better cloud-resource utilization.&lt;/li&gt; &lt;li&gt;Decreasing startup and scale-up time.&lt;/li&gt; &lt;li&gt;Minimizing the attack surface.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Runtime optimization with JBoss EAP, JBoss EAP XP, and Galleon&lt;/h2&gt; &lt;p&gt;We'll use &lt;a href="https://developers.redhat.com/products/eap/download"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) and JBoss EAP XP to decrease the size of our application image while also increasing container security. First, we'll develop a runtime image that eliminates development tools (such as Maven artifacts) that were present in the original &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/2/html/using_red_hat_software_collections_container_images/sti"&gt;Source-to-Image (S2I)&lt;/a&gt; environment. Then, we'll use &lt;a href="https://github.com/wildfly/galleon"&gt;Galleon&lt;/a&gt; to trim the application features and provide customization for JBoss EAP and its image’s footprint.&lt;/p&gt; &lt;p&gt;Note that we'll use the same &lt;a href="https://github.com/mvocale/JBoss_EAP_cloud_ready"&gt;GitHub repository&lt;/a&gt; we've used for the previous articles in the series. To start, switch to the &lt;code&gt;git&lt;/code&gt; tag that contains the source code used to implement the Galleon version:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ git checkout tags/Galleon_Runtime_version&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, delete the previous version of the application to start with a clean environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc delete all --selector app=weather-app-eap-cloud-ready $ oc delete is weather-app-eap-cloud-ready $ oc delete bc weather-app-eap-cloud-ready&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Import the image for the JBoss EAP XP 2.0 OpenJDK 11 runtime:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc import-image jboss-eap-7/eap-xp2-openjdk11-runtime-openshift-rhel8 --from=registry.redhat.io/jboss-eap-7/eap-xp2-openjdk11-runtime-openshift-rhel8 --confirm&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Update the buildConfig&lt;/h3&gt; &lt;p&gt;Now let's focus on the &lt;code&gt;buildConfig.yaml&lt;/code&gt; file under the &lt;code&gt;k8s&lt;/code&gt; directory. In that file, I defined a chained build with two &lt;code&gt;buildConfig&lt;/code&gt; objects: &lt;code&gt;weather-app-eap-cloud-ready-build-artifacts&lt;/code&gt; and &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt;. The first one is the S2I builder image that contains a complete JBoss EAP server with tooling needed during the S2I build. The second one has the runtime image that contains dependencies needed to run JBoss EAP. The first build creates the JBoss EAP XP instance and the application to be deployed, whereas the second build excludes the development tools not needed in the production environment. Figure 1 summarizes the components of the development process and their relationships.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/build_components.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/build_components.png?itok=BPVGhr6S" width="600" height="277" alt="Diagram with the steps needed to obtain a runtime image." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Components of builds for our containerized application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Here is a snapshot of the chained build:&lt;/p&gt; &lt;pre&gt; &lt;code class="yaml"&gt;kind: ImageStream apiVersion: image.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready-build-artifacts labels: application: weather-app-eap-cloud-ready-build-artifacts --- kind: ImageStream apiVersion: image.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready labels: application: weather-app-eap-cloud-ready --- kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: weather-app-eap-cloud-ready-build-artifacts namespace: redhat-jboss-eap-cloud-ready-demo labels: build: weather-app-eap-cloud-ready-build-artifacts spec: output: to: kind: ImageStreamTag name: 'weather-app-eap-cloud-ready-build-artifacts:latest' resources: {} strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: redhat-jboss-eap-cloud-ready-demo name: 'eap-xp2-openjdk11-openshift-rhel8:latest' source: type: Binary binary: {} --- kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: labels: application: weather-app-eap-cloud-ready name: weather-app-eap-cloud-ready spec: output: to: kind: ImageStreamTag name: weather-app-eap-cloud-ready:latest source: dockerfile: |- FROM eap-xp2-openjdk11-runtime-openshift-rhel8 COPY /server $JBOSS_HOME USER root RUN chown -R jboss:root $JBOSS_HOME &amp;&amp; chmod -R ug+rwX $JBOSS_HOME USER jboss CMD $JBOSS_HOME/bin/openshift-launch.sh images: - from: kind: ImageStreamTag name: weather-app-eap-cloud-ready-build-artifacts:latest paths: - sourcePath: "/s2i-output/server/" destinationDir: "." strategy: dockerStrategy: imageOptimizationPolicy: SkipLayers from: kind: ImageStreamTag name: eap-xp2-openjdk11-runtime-openshift-rhel8:latest namespace: redhat-jboss-eap-cloud-ready-demo type: Docker triggers: - imageChange: {} type: ImageChange&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Switch JBoss EAP XP to bootable JAR mode&lt;/h3&gt; &lt;p&gt;We'll also need to configure JBoss EAP XP to run in a bootable JAR mode so that you can enable the runtime image. To configure this mode, I set the following environment variables in the &lt;code&gt;environment&lt;/code&gt; file under the &lt;code&gt;.s2i&lt;/code&gt; directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;#GALLEON_PROVISION_DEFAULT_FAT_SERVER=true GALLEON_PROVISION_LAYERS=jaxrs-server,microprofile-platform S2I_COPY_SERVER=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can use the bootable JAR to build a bootable JAR application image, which contains a server, a packaged application, and the runtime required to launch the server. As shown in the YAML snippet, there are two properties related to the Galleon framework. The first one creates a bootable JAR with a full-featured JBoss EAP XP subsystem:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;GALLEON_PROVISION_DEFAULT_FAT_SERVER=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But our target is not only to have a slim and more secure container image that omits unnecessary tools. We also want to improve the use of cloud resources by removing unused subsystems from JBoss EAP XP. For this reason, I commented out the &lt;code&gt;GALLEON_PROVISION_DEFAULT_FAT_SERVER&lt;/code&gt; property. To include only the necessary subsystems, I also set the &lt;code&gt;GALLEON_PROVISION_LAYERS&lt;/code&gt; property with the names of the subsystems needed to run my application. The &lt;code&gt;jaxrs-server&lt;/code&gt; subsystem provides support for JAX-RS and JPA, while the &lt;code&gt;microprofile-platform&lt;/code&gt; subsystem includes the MicroProfile capabilities we added in Part 3.&lt;/p&gt; &lt;p&gt;I also set the property &lt;code&gt;S2I_COPY_SERVER &lt;/code&gt;to copy the result of the first build, named &lt;code&gt;weather-app-eap-cloud-ready-build-artifacts&lt;/code&gt; in the &lt;code&gt;buildConfig.yaml&lt;/code&gt;, into the final runtime image as described in the &lt;code&gt;weather-app-eap-cloud-ready&lt;/code&gt; build, which is always set in the &lt;code&gt;buildConfig.yaml&lt;/code&gt; file. Without this property, you can't complete this step.&lt;/p&gt; &lt;h3&gt;Create the new runtime image&lt;/h3&gt; &lt;p&gt;Now it’s time to create the &lt;code&gt;ImageStreams&lt;/code&gt; and the chained &lt;code&gt;buildConfig&lt;/code&gt; to make the runtime image with JBoss EAP XP 2 and the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc create -f k8s/buildConfig.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, start the build of the application on OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc start-build weather-app-eap-cloud-ready-build-artifacts --from-dir=. --wait&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I suggest that you check when the second build finishes with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get build weather-app-eap-cloud-ready-1 --watch&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the status moves from &lt;code&gt;Pending&lt;/code&gt; to &lt;code&gt;Complete&lt;/code&gt;, you can create the weather application for JBoss EAP XP 2 and configure it:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc create -f k8s/weather-app-eap-cloud-ready.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can then test your application, using the steps I have described in the previous articles, to verify that it is still working.&lt;/p&gt; &lt;h2&gt;Reviewing the outcomes&lt;/h2&gt; &lt;p&gt;Now it’s time to check the return on investment for the operations we've just performed. Figure 2 shows the new container image size.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/optimized_summary.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/optimized_summary.png?itok=eOtwHTxn" width="600" height="188" alt="The size of the final, optmized image is only 294.6 MB." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Summary and size information after our upgrade to remove unneeded components. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 3 shows the new memory footprint.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/optimized_memory.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/optimized_memory.png?itok=NwncFnza" width="600" height="40" alt="The final, optmized image uses only 718.9 MB of memory." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Memory use after our upgrade to remove unneeded components. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Consider these outcomes:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Container image size&lt;/strong&gt;: The previous application image, with Jakarta EE and MicroProfile features plus all of JBoss EAP XP and RHEL 8 UBI, takes up &lt;em&gt;455 MB&lt;/em&gt;. The final image, obtained through the optimizations we carried out in this article, is &lt;em&gt;294 MB&lt;/em&gt;, a savings of &lt;em&gt;35%&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory footprint&lt;/strong&gt;: The previous application release, with Jakarta EE and MicroProfile features, plus all of JBoss EAP XP and RHEL 8 UBI, requires &lt;em&gt;1,000 MB&lt;/em&gt; of memory. The final release, obtained through optimization, requires &lt;em&gt;718 MB&lt;/em&gt; of memory, a savings of &lt;em&gt;28%&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion to Part 4&lt;/h2&gt; &lt;p&gt;This series has gone through the steps to modernize a legacy Java EE application using Jakarta EE and Eclipse MicroProfile. The resulting final application includes features and services that are beneficial for microservice applications running in the cloud. By repeating the processes shown in the series, you can break your monolithic Java applications into small and independent modules without needing to heavily change your source code. The resulting runtime environment is:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Optimized for the cloud and containers&lt;/li&gt; &lt;li&gt;Lightweight, with a flexible architecture&lt;/li&gt; &lt;li&gt;More productive for developers&lt;/li&gt; &lt;li&gt;Flexible in management, configuration, and administration&lt;/li&gt; &lt;li&gt;Oriented to supporting and standardizing microservices development&lt;/li&gt; &lt;li&gt;Based entirely on open source tools and standards&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Don’t stop evolving all of your applications! Continuous improvement is the key to the success of your architecture.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;More about modernization&lt;/strong&gt;: &lt;a href="https://developers.redhat.com/articles/2021/06/14/application-modernization-patterns-apache-kafka-debezium-and-kubernetes"&gt;Application modernization patterns with Apache Kafka, Debezium, and Kubernetes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/02/making-java-programs-cloud-ready-part-4-optimize-runtime-environment" title="Making Java programs cloud-ready, Part 4: Optimize the runtime environment"&gt;Making Java programs cloud-ready, Part 4: Optimize the runtime environment&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6U-wqp9ynHw" height="1" width="1" alt=""/&gt;</summary><dc:creator>Mauro Vocale</dc:creator><dc:date>2021-07-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/02/making-java-programs-cloud-ready-part-4-optimize-runtime-environment</feedburner:origLink></entry><entry><title type="html">How to start contributing to Drools Executable Model</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Otx6EjlVX0g/how-to-start-contributing-to-drools-executable-model.html" /><author><name>Luca Molteni</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/Mql1NTVjhu8/how-to-start-contributing-to-drools-executable-model.html</id><updated>2021-07-01T13:45:17Z</updated><content type="html">WHAT IS THE EXECUTABLE MODEL? The Executable Model is a new way to execute business rules in Drools. It’s based on a Java representation of the rule structure that provides a few advantages such as faster startup time and better memory allocation at runtime. You can check out the details in the or in other blog posts such as . KJARs built with the kie-maven-plugin have the Executable Model enabled since 7.33.0.Final by default and it’s the main technology underneath . With the "Executable Model Compiler", a module you can find in the drools-model-compiler directory, DRL files are transformed into a Java DSL. HOW TO START CONTRIBUTING Drools is a really big open source project, and finding the best way to contribute to it might not be easy. Luckily, the Executable Model Compiler is a good way to start, for various reasons: * It’s a fairly new project (as today it’s been more or less three years since the inception) * It doesn’t require deep understanding of the Drools’ internal algorithm, PHREAK * There always is a former counterpart to verify the code against Regarding the third point, we want Drools to behave in the exact same way while using the former runtime (also called DRL) and the new one (called PatternDSL). CONTRIBUTING: SHOWING A PROBLEM Imagine that you’re interested in contributing to Drools, what should you do when you find a problem and you think it’s related to the Executable Model? Firstly we should understand where the problem is in Drools and if it’s eventually related to the Executable Model. To do that, we need to create the smallest piece of code that shows the problem: this is what we called a "bug reproducer" (also just "reproducer"). If you provide a bug report to , or the team will ask you to create such reproducer. There are two ways to do it: 1) If you’re familiar with the you can write the test directly in your own fork of the original repository and create a PR against it. This is probably the best way to proceed, as it allows all the Drools’ developers to check the problem faster 2) Create another separate project that shows the problem METHOD #1: CREATE A REPRODUCER IN DROOLS.GIT Start by building the Drools project reading the page. You can either decide to build it using or building only the module with mvn clean install. The second one is definitely faster. Once you have the project up and running you can open it with your preferred IDE and take a look at the tests in the drools-model-compiler module, for example org.drools.modelcompiler.CompilerTest. If you run these tests you’ll see they’re executed twice, once against the DRL mode and the other against the PatternDSL. It’s important that the tests run in both ways. If you see a difference in the execution, please create a PR. And if you want to fix it on your own, try – we love to see new contributors. METHOD #2: CREATE A SELF CONTAINED PROJECT Let’s use the Drools archetype and verify that your small reproducer is working against Drools Legacy. Start by creating a KJAR using the , modify the rules and the test to verify everything is working accordingly. The default archetype will run the test against the DRL mode. Change the generated model to build an executable model KJAR. To do so, switch in the pom.xml from drools-engine-classic to drools-engine. Also add the drools-model-compiler dependency. Compile the project using maven and the command line. Use mvn clean install -DskipTests=true as it’ll try run the tests using the classic engine but we don’t have the drools-mvel dependency in the class path anymore. Verify the Executable Model has been built in the KJAR, you can for example using this command to view the content inside the KJAR: jar -tf target/name_of_the_kjar.jar You will see all the Executable Model classes under and the drools-model file. Another way to do it is to check the maven log for this phrase: [INFO] Found 7 generated files in Canonical Model [INFO] Generating /Users/lmolteni/git/contribute/reproducer-kjar/target/generated-sources/drools-model-compiler/main/java/./org/example/P41/LambdaExtractor41A2683D222972683028514525A5437B.java ... Create another project called runner that has the original project as a dependency in the Maven pom.xml. The same archetype can be used again, but you have to change a few things * Remove all the classes in src/main/java * Remove the DRL files, as this project only consumes KJARs * Remove the kmodule.xml file * Remove the kie-maven-plugin from the build. Again, we don’t want to build KJARs here, only to use them. * Switch in the pom.xml from drools-engine-classic to drools-engine. * Remove the kjar packaging in pom.xml * Add the original KJAR dependency &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;reproducer-kjar&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; * Move the test from the original reproducer to this module * Never call kContainer.verify(); while using Executable Model KJARs as this will retrigger the build Run the test. This time the test will run using the Executable Model. You can see it because there will be this line in the logs 2021-06-15 11:32:29,576 INFO [org.drools.modelcompiler.CanonicalKieModuleProvider] (main) Artifact org.example:reproducer-kjar:1.0-SNAPSHOT has executable model This can probably be abstracted in a new archetype, let us know if you’re interested and we can work of it. SUMMARY In this article, we saw how to provide a small reproducer to verify an unexpected behaviour in Drools Executable Model and to provide the developers some way to verify the unexpected behaviour. Please try and contribute to Drools Executable Model! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Otx6EjlVX0g" height="1" width="1" alt=""/&gt;</content><dc:creator>Luca Molteni</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/Mql1NTVjhu8/how-to-start-contributing-to-drools-executable-model.html</feedburner:origLink></entry></feed>
